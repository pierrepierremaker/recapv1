import os
from io import BytesIO
from typing import List, Union
# from pydub import AudioSegment  <--- COMMENT√â POUR √âVITER L'ERREUR pyaudioop
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from openai.types.audio import Transcription

# --- NOUVELLE CONSTANTE POUR LA LIMITE DE TAILLE (API WHISPER) ---
MAX_FILE_SIZE = 25 * 1024 * 1024  # 25 Mo en bytes

# -----------------------
# 1. Chargement des variables d'environnement
# -----------------------
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    client = None
else:
    # Utilisation d'un client OpenAI standard
    # Remarque : si vous utilisez l'API Gemini, remplacez OpenAI par gemini.Client
    client = OpenAI(api_key=api_key) 

# -----------------------
# 2. Configuration de la page Streamlit
# -----------------------
st.set_page_config(
    page_title="Compte rendu de r√©union automatique",
    page_icon="üìù",
    layout="centered",
)

st.caption(f"üì¶ Taille max upload c√¥t√© Streamlit : {st.get_option('server.maxUploadSize')} Mo")

st.title("üìù G√©n√©rateur de compte rendu de r√©union")
st.warning(
    "‚ö†Ô∏è **Correction d'erreur `pyaudioop` :** Le d√©coupage audio automatique pour les fichiers de "
    "plus de 25 Mo est temporairement d√©sactiv√©. Veuillez n'uploader que des fichiers de **25 Mo maximum** "
    "jusqu'√† la prochaine mise √† jour."
)

# -----------------------
# 3. Upload du fichier audio
# -----------------------
uploaded_file = st.file_uploader(
    "D√©pose ton fichier audio ici (MP3 / WAV / M4A) - **MAX 25 Mo**",
    type=["mp3", "wav", "m4a"],
    help="Formats support√©s : MP3, WAV, M4A",
)

status_placeholder = st.empty()


# -----------------------
# 4. Fonctions utilitaires (SANS pydub)
# -----------------------

def transcribe_audio_simple(audio_file: BytesIO, language: str = "fr") -> str:
    """
    Transcrit un fichier audio unique (moins de 25 Mo) avec Whisper.
    L'objet doit √™tre un BytesIO (m√©moire) avec le nom de fichier correct.
    """
    if client is None:
        raise RuntimeError("Client OpenAI non initialis√© (cl√© API manquante).")

    # On s'assure que le pointeur est au d√©but pour l'API
    audio_file.seek(0)
    
    # L'API Whisper attend un objet de type fichier
    transcription: Transcription = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language=language,
    )
    return transcription.text


def estimate_whisper_cost(duration_minutes: float, price_per_minute_usd: float = 0.006) -> float:
    """
    Estime le co√ªt de transcription Whisper (whisper-1) en dollars.
    (La dur√©e doit √™tre estim√©e manuellement si pydub n'est pas utilis√©)
    """
    return duration_minutes * price_per_minute_usd


# -----------------------
# 5. Interface principale
# -----------------------
if uploaded_file is not None:
    st.success(f"‚úÖ Fichier charg√© : **{uploaded_file.name}**")
    
    # On convertit le fichier upload√© en objet BytesIO pour l'API
    audio_buffer = BytesIO(uploaded_file.getvalue())
    # On r√©assigne le nom pour que l'API reconnaisse le format
    audio_buffer.name = uploaded_file.name or "audio_file.mp3"
    
    # V√©rification simple de la taille
    file_size_mb = uploaded_file.size / (1024 * 1024)
    
    if uploaded_file.size > MAX_FILE_SIZE:
        st.error(
            f"‚ùå Fichier trop volumineux ({file_size_mb:.2f} Mo). "
            "La limite actuelle pour ce mode de transcription est 25 Mo."
        )
    else:
        st.write("Tu peux maintenant lancer la transcription audio ‚Üí texte.")
        
        # --------- 5.A ‚Äì Mode classique : Whisper (sans d√©coupage) ---------
        st.markdown("### üéß Transcription simple (Fichier unique)")
        
        # Pour le mode simple, on demande √† l'utilisateur d'estimer la dur√©e pour le co√ªt
        duration_minutes = st.number_input(
            "Dur√©e de la r√©union estim√©e (minutes)", 
            min_value=1.0, 
            value=min(file_size_mb * 2.5, 60.0), # Estimation grossi√®re
            step=5.0,
            help="Entre la dur√©e pour estimer le co√ªt (API Whisper : 0,006 $ / minute).",
        )
        
        estimated_cost = estimate_whisper_cost(duration_minutes)
        st.write(f"üí∞ Co√ªt estim√© de la transcription : ~**{estimated_cost:.4f} $**")
        
        if st.button("Transcrire la r√©union (Whisper)"):
            if client is None:
                st.error("‚ùå Aucune cl√© API OpenAI d√©tect√©e. Configure OPENAI_API_KEY pour continuer.")
            else:
                try:
                    status_placeholder.info("üó£Ô∏è Transcription en cours avec Whisper...")
                    
                    # 1) Transcription
                    full_transcript = transcribe_audio_simple(audio_buffer, language="fr")

                    status_placeholder.success("‚úÖ Transcription termin√©e !")

                    # 2) Affichage de la transcription
                    st.subheader("üßæ Transcription compl√®te")
                    st.write(
                        "Voici la transcription brute de la r√©union. "
                        "La prochaine √©tape (en bas) est la g√©n√©ration du compte rendu structur√©."
                    )
                    st.text_area(
                        "Transcription",
                        value=full_transcript,
                        height=400,
                    )

                    # On garde dans la session pour utilisation future (r√©sum√©, CR, etc.)
                    st.session_state["full_transcript"] = full_transcript
                    
                except Exception as e:
                    status_placeholder.error("‚ùå Erreur lors de la transcription.")
                    st.error(f"Une erreur est survenue lors de l'appel √† l'API Whisper : {str(e)}")


    # --------- 5.B ‚Äì Mode diarisation : gpt-4o-transcribe-diarize ---------
    # Le mode diarisation est naturellement limit√© √† 25 Mo, mais utilise un mod√®le diff√©rent (gpt-4o-transcribe-diarize)
    st.markdown("### üîä Transcription avec identification des locuteurs (Diarisation)")

    st.write(
        "Utilise ce mode si ton fichier fait **25 Mo ou moins**. "
        "Le mod√®le `gpt-4o-transcribe-diarize` est souvent plus performant pour identifier les locuteurs (A, B, C...)."
    )

    if st.button("Transcrire avec diarisation"):
        if client is None:
            st.error("‚ùå Aucune cl√© API OpenAI d√©tect√©e. Configure OPENAI_API_KEY pour continuer.")
        else:
            if uploaded_file.size > MAX_FILE_SIZE:
                st.error(
                    f"‚ùå Fichier trop volumineux pour la diarisation (taille : {file_size_mb:.1f} Mo). "
                    "La limite de l'API est 25 Mo. Utilise le mode simple si tu peux r√©duire la taille du fichier."
                )
            else:
                try:
                    with st.spinner("üß† Transcription + diarisation en cours..."):
                        # Le buffer est d√©j√† cr√©√© avec le contenu du fichier et le nom
                        
                        diarized = client.audio.transcriptions.create(
                            model="gpt-4o-transcribe-diarize",
                            file=audio_buffer,
                            response_format="diarized_json",
                            # chunking_strategy="auto", # Non n√©cessaire pour gpt-4o-transcribe-diarize, il le g√®re
                        )

                        # diarized.segments contient les segments avec speaker / start / end / text
                        segments = diarized.segments

                        # On construit un texte lisible
                        lines = []
                        for seg in segments:
                            speaker = seg.speaker
                            start = getattr(seg, "start", None)
                            end = getattr(seg, "end", None)
                            text = seg.text

                            if start is not None and end is not None:
                                lines.append(
                                    f"Speaker {speaker} [{start:.1f}s‚Äì{end:.1f}s] : {text}"
                                )
                            else:
                                lines.append(f"Speaker {speaker} : {text}")

                        labeled_transcript = "\n".join(lines)

                        st.success("‚úÖ Transcription diaris√©e termin√©e !")
                        st.subheader("üßæ Transcription avec locuteurs")
                        st.text_area(
                            "Texte diaris√© (qui parle, quand, quoi)",
                            value=labeled_transcript,
                            height=400,
                        )

                        # On garde √ßa dans la session pour le futur compte rendu
                        st.session_state["full_transcript"] = labeled_transcript

                except Exception as e:
                    st.error("‚ùå Erreur lors de la transcription avec diarisation.")
                    st.error(f"Une erreur est survenue lors de l'appel √† l'API : {str(e)}")

else:
    st.info("‚§¥Ô∏è Commence par d√©poser un fichier audio pour continuer.")

# -----------------------
# 6. G√©n√©ration du compte rendu avec GPT-4o-mini
# -----------------------
st.markdown("---")
st.subheader("üß† G√©n√©rer un compte rendu de la r√©union")

if "full_transcript" not in st.session_state:
    st.info("‚û°Ô∏è Transcris d'abord une r√©union (avec ou sans diarisation) pour pouvoir g√©n√©rer un compte rendu.")
else:
    transcript_text = st.session_state["full_transcript"]

    st.write(
        "√Ä partir de la transcription ci-dessus, l‚Äôoutil va produire un compte rendu synth√©tique, "
        "structur√© par th√®mes et par intervenant."
    )

    # Optionnel : rappel de la transcription (extrait)
    with st.expander("Voir un extrait de la transcription utilis√©e"):
        st.text_area(
            "Transcription (extrait)",
            value=transcript_text[:2000] + ("..." if len(transcript_text) > 2000 else ""),
            height=200,
        )

    style = st.selectbox(
        "Style de compte rendu",
        ["Professionnel / neutre", "Bullet points synth√©tiques", "Version d√©taill√©e (proc√®s-verbal)"],
        index=0,
    )

    if st.button("‚ú® G√©n√©rer le compte rendu"):
        if client is None:
            st.error("‚ùå Aucune cl√© API OpenAI d√©tect√©e. Configure OPENAI_API_KEY pour continuer.")
        else:
            try:
                with st.spinner("üß† R√©daction du compte rendu en cours..."):
                    # On adapte un peu le ton selon le style choisi
                    if style == "Professionnel / neutre":
                        style_instruction = (
                            "R√©dige un compte rendu professionnel, neutre, bien structur√©, en fran√ßais, "
                            "avec des titres et sous-titres clairs."
                        )
                    elif style == "Bullet points synth√©tiques":
                        style_instruction = (
                            "Fais un r√©sum√© tr√®s synth√©tique sous forme de listes √† puces, en fran√ßais, "
                            "en mettant surtout en avant les id√©es cl√©s et les chiffres importants."
                        )
                    else:  # Version d√©taill√©e (proc√®s-verbal)
                        style_instruction = (
                            "R√©dige un compte rendu d√©taill√©, proche d'un proc√®s-verbal, en fran√ßais, "
                            "en respectant fid√®lement le contenu sans inventer de faits."
                        )

                    system_msg = (
                        "Tu es un assistant charg√© de r√©diger des comptes rendus de r√©unions √† partir de transcriptions. "
                        "Tu dois √™tre clair, structur√©, fid√®le au contenu, et ne pas inventer de d√©cisions ou de chiffres. "
                        "Lorsque la transcription contient des √©tiquettes de locuteur comme 'Speaker A' ou 'Speaker B', "
                        "explique dans le compte rendu qui semble √™tre qui (ex : intervieweur, invit√©, expert...), "
                        "sans inventer d'identit√© r√©elle."
                    )

                    user_prompt = (
                        f"{style_instruction}\n\n"
                        "Voici la transcription de l'√©change (avec √©ventuellement des labels de locuteurs) :\n\n"
                        f"{transcript_text}\n\n"
                        "Produit maintenant le compte rendu demand√©."
                    )

                    # --- APPEL √Ä L'API DE R√âSUM√â ---
                    # Nous allons utiliser client.chat.completions.create qui est la m√©thode standard pour GPT
                    # L'API Gemini que vous utilisiez (client.responses.create) n'est pas standard pour OpenAI.
                    resp = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": user_prompt},
                        ],
                    )

                    # Le format de r√©ponse standard pour OpenAI Chat API
                    cr_texte = resp.choices[0].message.content 

                st.subheader("üìÑ Compte rendu g√©n√©r√©")
                st.write(cr_texte)

                # Option : on stocke le CR dans la session pour r√©utilisation ult√©rieure (export, etc.)
                st.session_state["meeting_summary"] = cr_texte
                
                st.download_button(
                    label="T√©l√©charger le compte rendu (Markdown)",
                    data=cr_texte,
                    file_name=f"compte_rendu_{uploaded_file.name.split('.')[0]}_CR.md",
                    mime="text/markdown"
                )

            except Exception as e:
                st.error("‚ùå Erreur lors de la g√©n√©ration du compte rendu.")
                st.error(f"Une erreur est survenue lors de l'appel √† l'API GPT-4o-mini : {str(e)}")